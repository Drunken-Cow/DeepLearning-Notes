<!DOCTYPE html>
<html>
<head>
<title>【笔记】机器学习&深度学习要点小梳理 - 幕布</title>
<meta charset="utf-8"/>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="renderer" content="webkit"/>
<meta name="author" content="mubu.com"/>
</head>
<body style="margin: 50px 20px;color: #333;font-family: 'Helvetica Neue','Hiragino Sans GB','WenQuanYi Micro Hei','Microsoft Yahei',sans-serif;">
<div class="export-wrapper"><div style="font-size: 24px; padding: 20px 15px 0;"><div style="padding-bottom: 10px">【笔记】机器学习&amp;深度学习要点小梳理</div><div style="background: #e5e6e8; height: 1px; margin-bottom: 20px;"></div></div><ul style="list-style: disc outside;"><li style="line-height: 28px;"><span class="content mubu-node" heading="2" style="line-height: 28px; min-height: 28px; font-size: 21px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">监督学习</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 24px;"><span class="content mubu-node" color="#dc2d1e" heading="3" style="color: rgb(220, 45, 30); line-height: 24px; min-height: 24px; font-size: 18px; display: inline-block; vertical-align: top;">（一）预测问题 --&gt; <span class="bold" style="font-weight: bold;">线性回归</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">单变量线性回归（单个特征）</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">模型的<span class="bold underline" style="font-weight: bold; text-decoration: underline;">假设函数</span>类似：h(x)=θ₀+θ₁x    参数：θ₀、θ₁ 未知数(特征)：x </span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold underline" style="font-weight: bold; text-decoration: underline;">cost function（损失函数、或称成本函数、或称代价函数）</span>：J(θ₀, θ₁) = 模型值与真实值误差</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">补充：选择正确的目标函数对解决问题是非常重要的。网络的目的是使损失尽可能最小化，因此，如果目标函数与成功完成当前任务不完全相关，那么网络最终得到的结果可能会不符合你的预期。想象一下，利用 SGD 训练一个愚蠢而又无所不能的人工智能，给它一个蹩脚的目标函数： “将所有活着的人的平均幸福感最大化”。为了简化自己的工作，这个人工智能可能会选择杀死绝大多数人类，只留几个人并专注于这几个人的幸福——因为平均幸福感并不受人数的影响。这可能并不是你想要的结果！因此，一定要明智地选择目标函数，否则你将会遇到意想不到的副作用。幸运的是，对于分类、回归、序列预测等常见问题，你可以遵循一些简单的指导原则来选择正确的损失函数。例如，对于二分类问题，你可以使用<span class="bold italic underline" style="font-weight: bold; text-decoration: underline; font-style: italic;">二元交叉熵（binary crossentropy）损失函数</span>；对于多分类问题，可以用<span class="bold italic underline" style="font-weight: bold; text-decoration: underline; font-style: italic;">分类交叉熵（categorical crossentropy）损失函数</span>；对于回归问题，可以用<span class="bold italic underline" style="font-weight: bold; text-decoration: underline; font-style: italic;">均方误差（mean-squared error）损失函数</span>；对于序列学习问题，可以用<span class="bold italic underline" style="font-weight: bold; text-decoration: underline; font-style: italic;">联结主义时序分类（CTC，connectionist temporal classification）损失函数</span>，等等。只有在面对真正全新的研究问题时，你才需要自主开发目标函数。——<span class="bold italic" style="font-weight: bold; font-style: italic;">来源《Python深度学习》</span></span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">目标：选择出可以使得损失函数最小的模型参数，即J(θ₀, θ₁)最小，使用的方法有：①<span class="bold" style="font-weight: bold;">梯度下降法</span> ②<span class="bold" style="font-weight: bold;">正规方程</span></span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">多变量线性回归（多个特征）</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">模型的假设类似：h(x)=θ₀+θ₁x₁+θ₂x₂+θ₃x₃+...+θnxn，该公式有 n+1 个参数和 n 个变量，目标使损失函数最小：</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">PS：线性回归并不适用于所有数据，有时我们需要曲线来适用我们的数据，比如一个二次方模型：H(x)=θ₀+θ₁x₁+θ₂x₂²，我们可以令：x₂=x₂²，x₃=x₃³，从而将模型转化为线性回归模型。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1. 梯度下降法</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">梯度下降法实践-<span class="bold" style="font-weight: bold;">特征缩放</span>：在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这奖帮助梯度下降算法更快地收敛</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">Xn =(Xn-Un)/Sn，其中Un是平均值，Sn是标准差。如 X₁ &lt;--- (X₁ -U₁ )/S1，U₁：在训练集中，特征值X₁的平均值，S₁：而S₁是该特征值的范围（这个范围可以是最大值减去最小值，或者可以是S₁的标准差）</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">梯度下降法实践-<span class="bold" style="font-weight: bold;">学习率</span>：梯度下降法算法每次迭代受到学习率的影响，如果学习率α过小，则达到收敛所需的迭代次数会非常高；如果学习率α过小，每次迭代可能不会减小待代价函数，可能会越过局部最小值导致无法收敛。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" images="%5B%7B%22w%22%3A421%2C%22ow%22%3A565%2C%22oh%22%3A116%2C%22id%22%3A%22384166b5b6a0b517a-456011%22%2C%22uri%22%3A%22document_image%2Ff8deafda-2e96-4b3e-881d-1b93977fc758-456011.jpg%22%7D%5D" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2. 正则方程：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/f8deafda-2e96-4b3e-881d-1b93977fc758-456011.jpg" style="max-width: 720px; width: 421px;" class="attach-img"></div><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">不需要选择学习率。</span></li></ul></li></ul></li></ul></li><li style="line-height: 24px;"><span class="content mubu-node" color="#dc2d1e" heading="3" style="color: rgb(220, 45, 30); line-height: 24px; min-height: 24px; font-size: 18px; display: inline-block; vertical-align: top;">（二）分类问题 --&gt; <span class="bold" style="font-weight: bold;">Logistic(逻辑)回归</span>、<span class="bold" style="font-weight: bold;">SVM（支持向量机）</span>等</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">逻辑回归</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">PS：若为多分类问题可以转为二分类思想解决。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">SVM（支持向量机）：代价函数为凸函数，不存在局部最小值。（更多内容不在这里记录了。）</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">PS：logistic回归、SVM等常用于解决二分类，softmax解决多分类</span></li></ul></li><li style="line-height: 24px;"><span class="content mubu-node" color="#dc2d1e" heading="3" style="color: rgb(220, 45, 30); line-height: 24px; min-height: 24px; font-size: 18px; display: inline-block; vertical-align: top;">（三）过拟合问题&amp;评估模型</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">我们做出了假设函数，一般采用选择参数量使误差最小，有人认为误差最小一定是件好事，......但并不不能说明一定是好的假设函数。PS：其实表达的意思就是说，模型在训练集表现非常好，但不一定是好的模型。因为很有可能模型只是在训练集表现很好，在新的数据集上就表现非常糟糕。这就是过拟合问题。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">我们需要有一种方式评估假设函数（评估我们的模型）过拟合检验。比如我们可以采用将数据分为训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据。</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">实际中，我们一般采用交叉验证集。把数据分为<span class="bold" style="font-weight: bold;">训练集</span>、<span class="bold" style="font-weight: bold;">验证集</span>、<span class="bold" style="font-weight: bold;">测试集</span>三部分。</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">①若训练集误差小，在验证集误差大，说明<span class="bold" style="font-weight: bold;">过拟/合</span>。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">②若训练集误差大，在验证集误差也很大，说明<span class="bold" style="font-weight: bold;">欠拟合</span>。</span></li></ul></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">发现过拟合问题？该如何处理呢？</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">①<span class="bold" style="font-weight: bold;">主成分分析（PCA）</span>：丢弃一些不能帮助我们正确预测的特征，可以是手工选择保留哪些特征，或者使用些模型选择的算法来帮忙，例如PCA。</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">主成分分析(PCA)是最常见的降维算法。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">对PCA的解释</span>：通过特征的组合来实现降维的方法。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">基本思想：在大部分实际问题中，需要考察的变量多，变量之间是有一定的相关性的，主成分分析就是以损失很少部分信息为代价，保留绝大部分信息的前提下，将原来众多具有一定线性相关性的p个指标压缩成少数几个互不相关的综合指标（主成分），并通过原来变量的少数几个的线性组合来给出各个主成分的具有实际背景和意义的解释。由于主成分分析浓缩了众多指标的信息，降低了指标的维度，从而简化指标的结构，深刻反映问题的内在规律。 </span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">相关资料：</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1. PCA主成分分析法原理分析：<a class="content-link" target="_blank" rel="noreferrer" href="https://wenku.baidu.com/view/6128821f5a8102d276a22f7f.html" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://wenku.baidu.com/view/6128821f5a8102d276a22f7f.html</a></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2. 主成分分析（PCA）原理详解：<a class="content-link" target="_blank" rel="noreferrer" href="https://blog.csdn.net/zhongkelee/article/details/44064401" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://blog.csdn.net/zhongkelee/article/details/44064401</a></span></li></ul></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">②<span class="bold" style="font-weight: bold;">正则化</span>：保留所有的特征，但是减少参数的大小。</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22ow%22%3A283%2C%22oh%22%3A51%2C%22id%22%3A%223ae166b5cf28ca162-456011%22%2C%22uri%22%3A%22document_image%2F9f1a2fad-9a0b-46dd-b27b-4192482c72de-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。 正则化线性回归的代价函数为：&nbsp;</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/9f1a2fad-9a0b-46dd-b27b-4192482c72de-456011.jpg" style="max-width: 720px;" class="attach-img"></div><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">正则化线性回归的梯度下降算法</span>的变化在于，每次都在原有算法更新规则的基础上令θ值减少了一个额外的值，达到惩罚特征目的。&nbsp;PS：其中 λ 称为正则化参数。对于正则化，我们要取一个合理的 λ 的值，这样才能更好的应用正则化。</span></li></ul></li></ul></li></ul></li></ul></li><li style="line-height: 24px;"><span class="content mubu-node" color="#dc2d1e" heading="3" style="color: rgb(220, 45, 30); line-height: 24px; min-height: 24px; font-size: 18px; display: inline-block; vertical-align: top;">（四）关于神经网络&amp;深度学习（来源：《Python深度学习》）</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A241%2C%22ow%22%3A308%2C%22oh%22%3A168%2C%22id%22%3A%22238166b9e423170bb-456011%22%2C%22uri%22%3A%22document_image%2Fb7f81152-ddc8-47f0-94f2-bfd5bdd8d870-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">1、机器学习是什么？</span>机器学习系统是训练出来的，而不是明确地用程序编写出来的。将与某个任务相关的许多示例输入机器学习系统，它会在这些示例中找到统计结构，从而最终找到规则将任务自动化。</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/b7f81152-ddc8-47f0-94f2-bfd5bdd8d870-456011.jpg" style="max-width: 720px; width: 241px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A301%2C%22ow%22%3A443%2C%22oh%22%3A338%2C%22id%22%3A%22c3166b9e80251086-456011%22%2C%22uri%22%3A%22document_image%2Ff50d0528-b537-4d8d-9b7e-ac829f7fcda8-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">2、深度学习工作原理</span></span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/f50d0528-b537-4d8d-9b7e-ac829f7fcda8-456011.jpg" style="max-width: 720px; width: 301px;" class="attach-img"></div><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">多个层链接在一起组成了网络，将输入数据映射为预测值。然后损失函数将这些预测值与目标进行比较，得到损失值，用于衡量网络预测值与预期结果的匹配程度。优化器使用这个损失值来更新网络的权重。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">一开始对神经网络的权重随机赋值，因此网络只是实现了一系列随机变换。其输出结果自然也和理想值相去甚远，相应地，损失值也很高。但随着网络处理的示例越来越多，权重值也在向正确的方向逐步微调，损失值也逐渐降低。这就是训练循环（training loop），将这种循环重复足够多的次数（通常对数千个示例进行数十次迭代），得到的权重值可以使损失函数最小。具有最小损失的网络，其输出值与目标值尽可能地接近，这就是训练好的网络。再次强调，这是一个简单的机制，一旦具有足够大的规模，将会产生魔法般的效果。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">3、深度学习之前：机器学习简史</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">深度学习已经得到了人工智能历史上前所未有的公众关注度和产业投资，但这并不是机器学习的第一次成功。可以这样说，当前工业界所使用的绝大部分机器学习算法都不是深度学习算法。深度学习不一定总是解决问题的正确工具：有时没有足够的数据，深度学习不适用；有时用其他算法可以更好地解决问题。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">3.1 概率建模</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">3.2 早期神经网络</span></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">虽然人们早在 20 世纪 50 年代就将神经网络作为玩具项目，并对其核心思想进行研究，但这一方法在数十年后才被人们所使用。在很长一段时间内，一直没有训练大型神经网络的有效方法。这一点在 20 世纪 80 年代中期发生了变化，当时很多人都独立地重新发现了反向传播算法——一种利用梯度下降优化来训练一系列参数化运算链的方法并开始将其应用于神经网络。</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">贝尔实验室于 1989 年第一次成功实现了神经网络的实践应用，当时 Yann LeCun 将卷积神经网络的早期思想与反向传播算法相结合，并将其应用于手写数字分类问题，由此得到名为 LeNet 的网络，在 20 世纪 90 年代被美国邮政署采用，用于自动读取信封上的邮政编码。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">3.3 核方法</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">3.4 决策树、随机森林与梯度提升机</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">3.5 回到神经网络</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2011 年，来自 IDSIA 的 Dan Ciresan 开始利用 GPU 训练的深度神经网络赢得学术性的图像分类竞赛，这是现代深度学习第一次在实践中获得成功。但<span class="bold" style="font-weight: bold;">真正的转折性时刻出现在 2012 年</span>，当年 Hinton 小组参加了每年一次的大规模图像分类挑战赛 ImageNet。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2012 年，由 Alex Krizhevsky 带领并由 Geoffrey Hinton 提供建议的小组在图像分类挑战赛 ImageNet，实现了 83.6% 的 top-5精度——这是一项重大突破。此后，这项竞赛每年都由深度卷积神经网络所主导。到了 2015 年，获胜者的精度达到了 96.4%，此时 ImageNet 的分类任务被认为是一个已经完全解决的问题。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#75c940" style="color: rgb(117, 201, 64); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">深度学习有何不同？</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">深度学习发展得如此迅速，主要原因在于它在很多问题上都表现出更好的性能。但这并不是唯一的原因。深度学习还让解决问题变得更加简单，因为<span class="bold" style="font-weight: bold;">它将特征工程完全自动化</span>，而这曾经是机器学习工作流程中最关键的一步。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">先前的机器学习技术（浅层学习）仅包含将输入数据变换到一两个连续的表示空间，通常使用简单的变换，比如高维非线性投影（SVM）或决策树。但这些技术通常无法得到复杂问题所需要的精确表示。因此，人们必须竭尽全力让初始输入数据更适合用这些方法处理，也必须手动为数据设计好的表示层。这叫作<span class="bold italic underline" style="font-weight: bold; text-decoration: underline; font-style: italic;">特征工程</span>。与此相反，深度学习完全将这个步骤自动化利用深度学习，你可以一次性学习所有特征，而无须自己手动设计。这极大地简化了机器学习工作流程，通常将复杂的多阶段流程替换为一个简单的、端到端的深度学习模型。</span></li></ul></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">4、深度学习用于计算机视觉的两个关键思想</span>，即卷积神经网络和反向传播，在 1989 年就已经为人们所知。长短期记忆（LSTM，long short-term memory）算法是深度学习处理时间序列的基础，它在 1997 年就被开发出来了，而且此后几乎没有发生变化。那么为什么深度学习在2012 年之后才开始取得成功？这二十年间发生了什么变化？总的来说，三种技术力量在推动着机器学习的进步：<span class="bold italic" style="font-weight: bold; font-style: italic;">①硬件</span>；<span class="bold italic" style="font-weight: bold; font-style: italic;">②数据集合基准</span>；<span class="bold italic" style="font-weight: bold; font-style: italic;">③算法上的改进</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">由于这一领域是靠实验结果而不是理论指导的，所以只有当合适的数据和硬件可用于尝试新想法时（或者将旧想法的规模扩大，事实往往也是如此），才可能出现算法上的改进。机器学习不是数学或物理学，靠一支笔和一张纸就能实现重大进展。它是一门工程科学。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">5、过拟合与欠拟合</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">机器学习的根本问题是优化和泛化之间的对立。优化（optimization）是指调节模型以在训练数据上得到最佳性能（即机器学习中的学习），而泛化（generalization）是指训练好的模型在前所未见的数据上的性能好坏。机器学习的目的当然是得到良好的泛化，但你无法控制泛化，只能基于训练数据调节模型。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">训练开始时，优化和泛化是相关的：训练数据上的损失越小，测试数据上的损失也越小。这时的模型是欠拟合（underfit）的，即仍有改进的空间，网络还没有对训练数据中所有相关模式建模。但在训练数据上迭代一定次数之后，泛化不再提高，验证指标先是不变，然后开始变差，即模型开始过拟合。这时模型开始学习仅和训练数据有关的模式，但这种模式对新数据来说是错误的或无关紧要的。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">为了防止模型从训练数据中学到错误或无关紧要的模式，最优解决方法是获取更多的训练数据。模型的训练数据越多，泛化能力自然也越好。如果无法获取更多数据，次优解决方法是调节模型允许存储的信息量，或对模型允许存储的信息加以约束。如果一个网络只能记住几个模式，那么优化过程会迫使模型集中学习最重要的模式，这样更可能得到良好的泛化。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">防止过拟合：</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">1. 减小网络大小</span>：防止过拟合的最简单的方法就是减小模型大小即减少模型中可学习参数的个数（这由层数和每层的单元个数决定）。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">2. 添加权重正则化</span>：一种常见的降低过拟合的方法就是强制让模型权重只能取较小的值，从而限制模型的复杂度，这使得权重值的分布更加规则（regular）。这种方法叫作权重正则化（weight regularization），其实现方法是向网络损失函数中添加与较大权重值相关的成本（cost）。这个成本有两种形式。</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">①&nbsp;L1 正则化（L1 regularization）：添加的成本与权重系数的绝对值［权重的 L1 范数（norm）］成正比。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">②L2 正则化（L2 regularization）：添加的成本与权重系数的平方（权重的 L2 范数）成正比。神经网络的 L2 正则化也叫权重衰减（weight decay）。不要被不同的名称搞混，权重衰减与 L2 正则化在数学上是完全相同的。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">3. 添加dropout正则化</span>：dropout 是神经网络最有效也最常用的正则化方法之一，它是由多伦多大学的 Geoffrey Hinton和他的学生开发的。对某一层使用 dropout，就是在训练过程中随机将该层的一些输出特征舍弃（设置为 0）。</span></li></ul></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">6、机器学习的通用工作流程</span>：定义问题、收集数据集、选择衡量成功的指标、确定评估方法、准备数据、开发比基准更好的模型、扩大模型规模：开发过拟合模型、模型正则化与调节超参数</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#ffaf38" style="color: rgb(255, 175, 56); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="italic" style="font-style: italic;">说明：《Python深度学习》该书为 keras 之父写的，对于机器学习、深度学习的理解和思考很到位和诚恳，推荐~</span></span></li></ul></li><li style="line-height: 24px;"><span class="content mubu-node" color="#dc2d1e" heading="3" style="color: rgb(220, 45, 30); line-height: 24px; min-height: 24px; font-size: 18px; display: inline-block; vertical-align: top;">（五）关于深度学习历史&amp;激活函数&amp;常见神经网络结构等（来源：廖星宇《深度学习入门之PyTorch》）</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">1、机器学习</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">人工智能的概念很宽泛，现在根据人工智能将它分为三大类。（我们现在处在一个充满弱人工智能的世界）</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1. 弱人工智能</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2. 强人工智能</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">3. 超人工智能</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">机器学习算是实现人工智能一种途径。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">机器学习更加注重算法的设计，让计算机能够自动地从数据中“学习”规律，并利用规律对未知数据进行预测。因为学习算法涉及了大量的统计学理论，与统计推断联系尤为紧密，所以也被称为统计学习方法。机器学习可分类为以下 5 个大类：</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1. 监督学习：从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归和分类。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2. 无监督学习：无监督学习与监督学习想必，训练集没有人为标注的结果。常见的无监督学习算法有聚类等。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">3. 半监督学习：这是一种介于监督学习与无监督学习之间的方法。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">4. 迁移学习：将已经训练好的模型参数迁移到新的模型来帮助新模型训练数据集。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">5. 增强学习：通过观察周围环境来学习。每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">传统机器学习算法有以下几种：<span class="bold italic" style="font-weight: bold; font-style: italic;">线性回归模型、logistic回归模型、k-临近算法、决策树、随机深林、支持向量机、人工神经网络、EM算法、概率图模型</span>等。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">2、深度学习历史</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">深度学习的最初级版本是人工神经网络，这是机器学习的一个分支。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A394%2C%22ow%22%3A559%2C%22oh%22%3A374%2C%22id%22%3A%22399166bad3cba6143-456011%22%2C%22uri%22%3A%22document_image%2Fa4b0bc97-8866-47e1-8227-4d125b0cc7e0-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">一张图来概括一下深度学习的历史浪潮</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/a4b0bc97-8866-47e1-8227-4d125b0cc7e0-456011.jpg" style="max-width: 720px; width: 394px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold underline" style="font-weight: bold; text-decoration: underline;">第一代神经网络</span>（1958年—1969年）</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">最早的神经网络思想起源于1943年的MCP人工神经元模型，当时人们希望能够用计算机来模型人的神经元反应的过程，该模型将神经元简化为三个过程：输入信号线性加权，求和，非线性激活（阀值法）。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">第一次将MCP用于机器学习（分类）的当属于1958年Rosenblatt发明的感知器算法。该算法使用MCP模型对输人的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年，该方法被证明为能够收敛，它的理论与实践效果引发了第一次神经网络的浪潮。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">然而学科发展的历史不总是一帆风顺的。1969年，美国数学家及人工智能先驱Mmsky在其著作中<span class="bold italic" style="font-weight: bold; font-style: italic;">证明了感知器本质上是一种线性模型，只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确分类</span>。这等于直接宣判了感知器的死刑，神经网络的研究也陷人了近20年的停滞。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold underline" style="font-weight: bold; text-decoration: underline;">第二代神经网络</span>（1986年—1998年）</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">第一次打破非线性诅咒的当属现代深度学习大牛Hinton，他在1986年发明了适用于<span class="bold italic" style="font-weight: bold; font-style: italic;">多层感知器（MLP）的BP算法</span>，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引发了神经网络的第二次热潮。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1989年，Robert Hecht-Nie1sen<span class="bold italic" style="font-weight: bold; font-style: italic;">证明了MLP的万能逼近定理</span>，即<span class="bold italic underline" style="font-weight: bold; text-decoration: underline; font-style: italic;">对于任何闭区间内的一个连续函数 f ，都可以用含有一个隐含层的BP网络来逼近</span>。该定理的发现，极大地鼓舞了神经网络的研究人员。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">也是在1989年，LeCun发明了卷积神经网一一LeNet，并将其用于数字识别，且取得了较好的成绩，不过当时并没有引起足够的注意。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">值得强调的是，1989年以后由于没有提出特别突出的方法，且神经网络一直缺少相应的严格的数学理论支持，神经网络的热潮渐渐冷淡下去。冰点发生于1991年，BP算法被指出存在梯度消失问题，即在误差梯度后向传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该发现对此时的神经网络发展无异于雪上加霜。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1997年，LSTM模型被发明，尽管该模型在序列建模上表现出的特性非常突出，但由于正处于神经网络的下坡期，也没有引起足够的重视。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold underline" style="font-weight: bold; text-decoration: underline;">统计学习方法的春天</span>（1986年—2006年）</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1986年，决策树方法被提出，很快ID3，ID4，CART等改进的决策方法相继出现（到目前仍然是非常常用的机器学习方法），这些方法也是符号学习方法的代表。正是由于这些方法的出现，使得统计学习开始进人人们的视野，迎来统计学习方法的春天。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1995年，统计学家vapnik提出线性SVM。由于它有非常完美的数学理论推导做支撑（统计学与凸优化等），并且非常符合人的直观感受（最大间隔），逐渐成为当时的主流算法，更重要的是它在线性分类的间题上取得了当时最好的成绩，这使得神经网络更陷人无人问津的境地。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1997年，AdaBoost被提出，该方法是PAC（Probably Approxrmately Correct）理论在机器学习实践上的代表，也催生了集成学习（EnsembleLearmng）这一类方法的诞生，在回归和分类任务上取得了非常好的效果：该方法通过一系列的弱分类器集成，达到强分类器的效果；现在集成学习仍然活跃在传统机器学刁方法中，在很多比赛中依旧大放异彩。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2000年，Kemel SVM被提出，核化的SVM通过一种巧妙的方式将原空间线性不可分的问题，通过Kemel映射成高维空间的线性可分问题，成功解决了非线性分类的问题，且分类效果非常好。至此也更加终结了神经网络时代，在有着如此多完美理论支持的方法中，神经网络似乎被宣告了死刑。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2001年、随机森林被提出，这是集成方法的另一代表，该方法的理论扎实，比AdaBoost能更好地抑制过拟合问题，实际效果也非常不错，使得机器学习更向前迈进一步。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2001年，一种新的统一框架一一图模型被提出，该方法试图统一机器学习混乱的方法，如朴素贝叶斯、SVM、隐马尔可夫模型等，为各种学习方法提供一个统一的描述框架，希望实现大一统的理论框架。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold underline" style="font-weight: bold; text-decoration: underline;">第三代神经网络</span>（2006年至今）</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">快速发展期（2006年—2012年）</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2006年，深度学习元年。这一年，Hinton提出了深层网络训练中梯度消失问题的解决方案：“无监督预训练对权值进行初始化+有监督训练微调"。其主要思想是先通过自学习的方法学习到训练数据的结构（自动编码器），然后在该结构上进行有监督训练微调。但是由于没有特别有效的实验验证，该论文并没有引起重视。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2011年，ReLU激活数被提出，该激活函数能够有效地抑制梯度消失的问题。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2011年，微软首次将深度学习应用在语音识别上，取得了重大突破。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">爆发期（2012年至今）</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，通过构建CNN网络AIexNet一举夺得冠军，并且碾压了第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引了众多研究者的注意。</span></li></ul></li></ul></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">3、深度学习框架？</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">在深度学习阶段初始阶段，每个深度学习研究者都需要写大量的重复代码。为了提高工作效率，这些研究者就将这些代码写成一个框架放到网上让所有研究者一起使用，接着网上就出现了不同的框架。随着时间的推移，最为好用的几个框架被大量的人使用从而流行了起来。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">TensorFlow：这是一款使用C++语言开发的开源数学计算软件，使用数据流图（Data Flow Graph）的形式进行计算。图中的节点代表数学运算，而图中的线条表示多维数据数组（tensor）之间的交互。TensorFlow灵活的架构可以部署在一个或多个CPU、GPU的台式及服务器中，或者使用单一的API应用在移动设备中。由于其语言太过于底层，目前有很多基于TensorFlow的第三方臭抽象库将TensorFlow的函数进行封装，使其变得简洁，目前比较有名的几个是 Keras、TFlearn、ftslim，以及TensorLayer。TensorFlow最初是由研究人员和Google Brain团队针对机器学习和深度神经网络进行研究开发的，目前开源之后几乎可以在各种领域适用。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">Caffe：和 Tensorflow 名气一样大的是深度学习框架Caffe，由加州大学伯克利的 Phd 贾扬清开发，全称是 ConvolutionalArchitectureforFastFeatureEmbedding，是一个清晰而高效的开源深度学习框架，目前由伯克利视觉学中心（BerkeleyVisionandLearningCenter，BVLC）进行维护。从它的名字就可以看出其对于卷积网络的支持特别好，同时也是用C++写的，但是并没有提供Python接口，只提供了C++的接口。Caffe之所以流行，是因为之前很多ImageNet比赛里面使用的网络都是用Caffe写的，所以如果你想使用这些比赛里面的网络模型就只能使用Caffe，这也就导致了很多人直接转到Caffe这个框架下面。Caffe的缺点是不够灵活，同时内存占用高，只提供了C++的接口。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">Theano：Theano于2008年诞生于蒙特利尔理工学院，其派生出了大量深度学习 Python 软件包，最著名的包括 BIocks 和 Keras。Theano的核心是一个数学表达式的编译器，它知道如何获取你的结构，并使之成为一个使用 numpy、高效本地库的高效代码，如BLAS和本地代码（C++）在CPU或GPU上尽可能快地运行。它是为深度学习中处理大型神经网络算法所需的计算而专门设计的，是这类库的首创之一（发展始于2007年），被认为是深度学习研究和开发的行业标准但是目前开发Theano的研究人员大多去了Google参与Tensorflow的开发，所以某种程度来讲Tensorflow就像Theano的孩于。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">Torch：Torch是一个有大量机器学习算法支持的科学计算框架，其诞生己经有十年之久，但是真正起势得益于Facebook开源了大量Toreh的深度学习模块和扩展。Torch的特点在于特别灵活，但是另外一个特殊之处是采用了编程语言Lua，在目前深度学习大部分以Python为编程语言的大环境之下，一个以Lua为编程语言的框架有着更多的劣势，这一项小众的语言增加了学习使用Torch这个框架的成本。</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">PS：PyTorch的前身便是Torch，其底层和Torch框架一样，但是使用Python重新写了很多内容，不仅更加灵活，支持动态图，也提供了Python接口。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">MXNet：MXNet的主要作者是李沐，最早就是几个人抱着纯粹对技术和开发的热情做起来的兴趣项目，如今成为了亚马逊的官方框架，有着非常好的分布式支持，而且性能持别好，占用显存低，同时其开放的语言接口不仅仅有Python和C++，还有R，Matlab，Scala，JavaScrrpt，等等，可以说能够满足使用任何语言的人。但是MXNet的缺点也很明显，教程不够完善，使用的人不多导致社区不大，同时每年很少有比赛和论文是基于MXNet实现的，这就使得MXNet的推广力度和知名度不高。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">PyTorch：PyTorch是 Torch7 团队开发的，从它的名字就可以看出，其与 Torch 的不同之处在于 PyTorchf 用了Python作为开发语言所谓"Python First”，同样说明它是一个以Python优先的深度学习框架，不仅能够实现强大的GPU加速，同时还支持动态神经网络，这是现在很多主流框架比如TensorFlow等都不支持的。</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#797ec9" style="color: rgb(121, 126, 201); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">为何使用PyTorch？</span></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">（1）掌握一个框架并不能一劳永逸，现在深度学习并没有谁拥有绝对的垄断地位，就算是Google也没有，所以只学习Tensorflow并不够。同时现在的研究者使用各个框架的都有，如果你要去看他们实现的代码，至少也需要了解他们使用的框架，所以多学一个框架，以备不时之需。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">（2）TensorFlow 与 Caffe 都是命令式的编程语言，而且是静态的，首先必须构建一个神经网络，然后一次又一次使用同样的结构，如果想要改变网络的结构，就必须从头开始。但是对于 PyTorch，通过一种反向自动求导的技术，可以让你零延迟地任意改变神经网络的行为，尽管这项技术不是PyTorch独有，但目前为止它实现是最快的，能够为你任何疯狂想法的实现获得最高的速度和最佳的灵活性，这也是 PyTorch 对比 Tensorflow 最大的优势。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">（3）PyTorch 的设计思路是线性、直观且易于使用的，当你执行一行代码时，它会忠实地执行，并没有异步的世界观，所以当你的代码出现 Bug 的时候，可以通过这些信息轻松快捷地找到出错的代码，不会让你在的时候因为错误的指向或者异步和不透明的引擎浪费太多的时间。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">（4）PyTorch 的代码相对于 Tensorflow 而言，更加简洁直观，同时对于 Tensorflow 高度工业化的很难看懂的底层代码，PyTorch 的源代码就要友好得多，更容易看懂。深人 API，理解 PyTorch 底层肯定是一件令人高兴的事。一个底层架构能够看懂的框架，你对其的理解会更深。</span></li></ul></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">4、其他内容梳理</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">（1）激活函数</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A423%2C%22ow%22%3A477%2C%22oh%22%3A37%2C%22id%22%3A%22151166bdcbe2df104-456011%22%2C%22uri%22%3A%22document_image%2F5b5eccb6-8741-4702-b298-a86577a4a184-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">①<span class="bold underline" style="font-weight: bold; text-decoration: underline;">Simoid</span></span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/5b5eccb6-8741-4702-b298-a86577a4a184-456011.jpg" style="max-width: 720px; width: 423px;" class="attach-img"></div><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A295%2C%22ow%22%3A472%2C%22oh%22%3A301%2C%22id%22%3A%2238f166bdcc57fc049-456011%22%2C%22uri%22%3A%22document_image%2F801fa2e3-cbf2-42a3-82ab-88f2ce7243b9-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">函数图像表达式：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/801fa2e3-cbf2-42a3-82ab-88f2ce7243b9-456011.jpg" style="max-width: 720px; width: 295px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1）Sigmoid 函数会造成梯度消失：一个非常不好的特点就是 Sigmoid 函数数在靠近 1 和 0 的两端时，梯度会几乎变成 0，我们前面讲过梯度下降法通过梯度乘上学习率来更新参数，因此如果梯度接近 0，那么没有任何信息来更新参数，这样就会造成模型不收敛。另外，如果使用 Sigmoid 数，那么需要在初始化权重的时候也必须非常小心。如果初始化的时候权重太大，那么经过激活函数也会导致大多数神经元变得饱和，没有办法更新参数。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2）Sigmoid 输出不是以 0 为均值，这就会导致经过 Sigmoid 激活函数之后的输出，作为后面一层网络的输人的时候是非 0 均值的，这个时候如果输入进人下一层神经元的时候全是正的，这就会导致梯度全是正的，那么在更新参数的时候永远都是正梯度。怎么理解呢？比如进人下一层神经元的输人是 x， 参数是 w 和 b，那么输出就是f=wx+b，这个时候 ▽f(w)=x，所以如果是 0 均值的数据，那么梯度就会有正有负。但是这个问题并不是太严重，因为一般神经网络在训练的时候都是按 batch（批）进行训练的，这个时候可以在一定程度上缓解这个问题，所以说虽然 0 均值这个问题会产生一些不好的影响，但是总体来讲跟上一个缺点：梯度消失相比还是要好很多。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A317%2C%22ow%22%3A434%2C%22oh%22%3A272%2C%22id%22%3A%2255166bdd4a85e19-456011%22%2C%22uri%22%3A%22document_image%2Fdc5ebc8c-0b76-4419-8c2b-9b4f06d8ce42-456011.jpg%22%7D%2C%7B%22w%22%3A317%2C%22ow%22%3A283%2C%22oh%22%3A86%2C%22id%22%3A%222a7166c280c4c7019-456011%22%2C%22uri%22%3A%22document_image%2Fcdb1be93-b14f-47ae-a216-d75e0042d39e-456011.jpg%22%7D%2C%7B%22w%22%3A317%2C%22ow%22%3A434%2C%22oh%22%3A272%2C%22id%22%3A%22309166c2827ada11d%22%2C%22uri%22%3A%22document_image%2Fdc5ebc8c-0b76-4419-8c2b-9b4f06d8ce42-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">②<span class="bold underline" style="font-weight: bold; text-decoration: underline;">Tanh</span></span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/dc5ebc8c-0b76-4419-8c2b-9b4f06d8ce42-456011.jpg" style="max-width: 720px; width: 317px;" class="attach-img"></div><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/cdb1be93-b14f-47ae-a216-d75e0042d39e-456011.jpg" style="max-width: 720px; width: 317px;" class="attach-img"></div><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/dc5ebc8c-0b76-4419-8c2b-9b4f06d8ce42-456011.jpg" style="max-width: 720px; width: 317px;" class="attach-img"></div><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A164%2C%22ow%22%3A283%2C%22oh%22%3A86%2C%22id%22%3A%22244166c281072c188-456011%22%2C%22uri%22%3A%22document_image%2F13d4b202-c122-438e-974a-96f4a6b74204-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">数学表达式：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/13d4b202-c122-438e-974a-96f4a6b74204-456011.jpg" style="max-width: 720px; width: 164px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">它将输入的数据转化到 -1 ~ 1 之间，可以通过图像看出它将输出变成了 0 均值，在一定程度上解决了 Sigmoid 函数的第二个问题，但是它仍然存在梯度消失的问题。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A323%2C%22ow%22%3A400%2C%22oh%22%3A263%2C%22id%22%3A%22395166bdd706870ea-456011%22%2C%22uri%22%3A%22document_image%2Fa431cd07-c133-4765-95ec-fbc8cf1eaea2-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">③<span class="bold underline" style="font-weight: bold; text-decoration: underline;">ReLU</span></span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/a431cd07-c133-4765-95ec-fbc8cf1eaea2-456011.jpg" style="max-width: 720px; width: 323px;" class="attach-img"></div><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">ReLU 激活函数（Rectified Linear Unit）近几年变得越来越流行，数学表达式为 <span class="bold" style="font-weight: bold;">f(x)=max(0, x)</span>，换句话说，这个激活函数只是简单地将大于 0 的部分保留，将小于 0 的部分变成 0,。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">④<span class="bold underline" style="font-weight: bold; text-decoration: underline;">Leaky ReLu</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A312%2C%22ow%22%3A348%2C%22oh%22%3A245%2C%22id%22%3A%22248166c281a119084-456011%22%2C%22uri%22%3A%22document_image%2F6dcc370a-12f8-4fe2-b1eb-48d026bdfbde-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">Leaky ReLU 对 ReLU 进行了改进，其图形表达式如下所示：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/6dcc370a-12f8-4fe2-b1eb-48d026bdfbde-456011.jpg" style="max-width: 720px; width: 312px;" class="attach-img"></div></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">⑤<span class="bold underline" style="font-weight: bold; text-decoration: underline;">Maxout</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A311%2C%22ow%22%3A414%2C%22oh%22%3A63%2C%22id%22%3A%2246166c2849d15127-456011%22%2C%22uri%22%3A%22document_image%2F4c4df1b2-35f4-4154-8ff0-b52c3392ab90-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">Maxout 最早出现在 ICML2013 上，由 Goodfellow 提出。其表达式如下所示：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/4c4df1b2-35f4-4154-8ff0-b52c3392ab90-456011.jpg" style="max-width: 720px; width: 311px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">Maxout 的拟合能力是非常强的，它可以拟合任意的的凸函数。最直观的解释就是任意的凸函数都可以由分段线性函数以任意精度拟合，而 Maxout 又是取 k 个隐藏层节点的最大值，这些”隐藏层"节点也是线性的，所以在不同的取值范围下，最大值也可以看做是分段线性的。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">Maxout 保证了始终是线性区域，没有饱和区，训练速度快，而且不会出现坏死神经元。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">⑥<span class="bold underline" style="font-weight: bold; text-decoration: underline;">ELU</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22ow%22%3A365%2C%22oh%22%3A228%2C%22id%22%3A%223b3166c28448d4017-456011%22%2C%22uri%22%3A%22document_image%2F6e1dfdf4-18fe-430e-8adb-4949581ca399-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">ELU（Exponential Linear Units）也是 ReLU 的一个变种，其图形表达式如下所示：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/6e1dfdf4-18fe-430e-8adb-4949581ca399-456011.jpg" style="max-width: 720px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A220%2C%22ow%22%3A346%2C%22oh%22%3A95%2C%22id%22%3A%22305166c289f33c0d5-456011%22%2C%22uri%22%3A%22document_image%2F39a57a2a-8dbe-451c-b3fa-46f5474691fc-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">数学表达式：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/39a57a2a-8dbe-451c-b3fa-46f5474691fc-456011.jpg" style="max-width: 720px; width: 220px;" class="attach-img"></div></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold italic" style="font-weight: bold; font-style: italic;">如何选择合适的激活函数？</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1）首选 ReLU，速度快，但是要注意学习速率的调整，</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2）如果 ReLU 效果欠佳,尝试使用 Leaky ReLU、ELU 或 Maxout 等变种。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">3）可以尝试使用 tanh。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">4）Sigmoid 和 tanh 在 RNN（LSTM、注意力机制等）结构中有所应用，作为门控或者概率值。其它情况下，减少 Sigmoid 的使用。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">5）在浅层神经网络中，选择使用哪种激励函数影响不大。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">推荐阅读「<span class="italic" style="font-style: italic;">6 种激活函数核心知识点，请务必掌握</span>」：<a class="content-link" target="_blank" rel="noreferrer" href="https://mp.weixin.qq.com/s/Cvf3ReuCKSugp9lrZqlaqg" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://mp.weixin.qq.com/s/Cvf3ReuCKSugp9lrZqlaqg</a>、「<span class="italic" style="font-style: italic;">理解神经网络的激活函数</span>」：<a class="content-link" target="_blank" rel="noreferrer" href="https://mp.weixin.qq.com/s/cf4qnGJgmqNlzaN6OJqdlA" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://mp.weixin.qq.com/s/cf4qnGJgmqNlzaN6OJqdlA</a></span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">（2）神经网络模型的表示能力与容量</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">前面我们通过脑神经结构引出了神经网络的层结构，如果从数学的角度来解释神经网络，那么神经网络就是由网络中的<span class="bold italic underline" style="font-weight: bold; text-decoration: underline; font-style: italic;">参</span><span class="bold underline" style="font-weight: bold; text-decoration: underline;">数决定的函数簇</span><span class="bold" style="font-weight: bold;">。</span>所谓的函数簇就是一系列的函数，这些函数都是由网络的参数决定的。提出了函数簇之后，我们就想明确这个函数簇的表达能力，也就是我们想知道是否有函数是这个函数簇无法表达的？</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">这个问题在 1989 年就被人证明过，<span class="bold underline" style="font-weight: bold; text-decoration: underline;">拥有至少一个隐藏层的神经网络可以逼近任何的连续函数</span>。如果一个只有一层隐藏层的神经网络就能够逼近任何连续函数，为什么我们还要使用更多层的网络呢？这个问题可以这么去理解，理论上讲增加的网络层可以看成是一系列恒等变换的网络层，也就是说这些网络层对输人不做任何变换，这样这个深层的网络结构至少能够达到与这个浅层网络相同的效果；同时在实际使用中我们也发现更深层的网箔具有更好的表现力，同时有着更好的优化结果。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">（3）梯度下降法的变式</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">①SGD：随机梯度下降法是梯度下降法的一个小变形，就是每次使用一批（batch）数据进行梯度的计算，而不是计算全部数据的梯度。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">②Momentum：第二种优化方法就是在随机梯度下降的同时，增加动量（Momentum）。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">③Adagrad：这是一种自适应学习率（adaptive）的方法。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">④RMSprop：这是一种非常有效的自适应学习率的改进方法。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">⑤Adam：这是一种综合型的学习方法，可以看成是 RMSprop 加上动量（Momentum）的学习方法，达到比 RMSProp 更好的效果。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">实际中，我们可以使用 Adam 作为默认的优化算法，往往能够达到好的效果，同时 SGD+Momentum 的方法也值得尝试。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">（4）数据预处理</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">①中心化：每个特征维度都减去相应的均值实现中心化，这样可以使得数据变成 0 均值。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">②标准化：在使得数据都变成 0 均值之后，还需要使用标准化的做法让数据不同的特征维度都有着相同的规模。有两种常用的方法：一种是除以标准差，这样可以使得新数据的分布接近标准高斯分布；还有一种做法就是让每个特征维度的最大值和最小值按比例缩放到 -1～1 之间。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">③PCA：PCA是另外一种处理数据的方法，在进行这一步以前，首先会将数据中心化，然后计算数据的协方差矩阵......</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">④白噪声：白噪声也是一种处理数据的方式，首先会跟PCA一样将数据投影到一个特征空间，然后每个维度除以特征值来标准化这些数据，直观上就是一个多元高斯分布转换到了一个 0 均值，协方差矩阵为 1 的多元高斯分布。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">（5）权重初始化</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">①全 0 初始化</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">②随机初始化</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">③稀疏初始化</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">④初始化偏置（bias）</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">⑤批标准化（Batch Normalization）</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">（6）防止过拟合</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">①正则化：L1正则化、L2正则化</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">②Dropout</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">（7）关于卷积神经网络（CNN）</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1. 局部性：对于一张图片而言，需要检测图片中的特征来决定图片的类别，通常情况下这些特征都不是由整张图片决定的，而是由一些局部的区域决定的。比如图中的鸟喙，该特怔只存在于图片的局部中。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2. 相同性：对于不同的图片，如果它们具有同样的特征，这些特征会出现在图片不同的位置，也就是说可以用同样的检测模式去检测不同图片的相同特证，只不过这些特征处于图片中不同的位置，但是特征检测所做的操作几乎一样。比如图中两张图片的鸟喙处于不同的位置，但是可以用相同的检测模式去检测。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">3. 不变形： 对于一张大图片，如果我们进行小采样，那么图片的性质基本保持不变。比如下图经过下采样还是能够看出来是一张鸟的图片。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">上面的三个性质分别对应着卷积神经网络中的三个思想。（<span class="bold italic underline" style="font-weight: bold; text-decoration: underline; font-style: italic;">局部连接、权值共享、池化采样</span>）</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">（8）一些常见卷积神经网络</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1. <span class="bold underline" style="font-weight: bold; text-decoration: underline;">LeNet</span>：LeNet是整个卷积神经网络的开山之作，1998 年有 LeCun 提出，它的结构特别简单，一共有 7 层，其中 2 层卷积核 2 层池化层交替出现，最后输出 3 层全连接层得到整体的结果。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2. <span class="bold underline" style="font-weight: bold; text-decoration: underline;">AleNet</span>：在 2012 年 ImageNet 竞赛上面大放异彩的 AleNet，它以领先第二名 10% 的准确率夺得冠军，并且成功地向世界展示了深度学习的威力。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">3. <span class="bold underline" style="font-weight: bold; text-decoration: underline;">VGGNet</span>：VGGNet 是 ImageNet 2014 年的亚军，总结起来就是它使用了更小的滤波器，同时使用了更深的结构，AlexNet 只有 8 层wangle，而 VGGNet 有 16~19 层网络，也不像 AlexNet 使用 11*11 那么大的滤波器，它只使用 3*3 的卷积滤波器和 2*2 的大池化层。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">4. <span class="bold underline" style="font-weight: bold; text-decoration: underline;">GoogleNet</span>：GoogleNet 也叫 InceptionNet，是在 2014 年被提出的，如今已经进化到了 v4 版本。它采取了比 VGGNet 更深的网络，一共有 22 层，但是它的参数却比 AlexNet 少了 12 倍，同时有很高的计算效率，因为它采用了一种很有效的 lnception 模块，而且它也没有全连接层，是 2014 年比赛的冠军。</span></li><li style="line-height: 22px;"><span class="content mubu-node" images="%5B%7B%22w%22%3A485%2C%22ow%22%3A665%2C%22oh%22%3A296%2C%22id%22%3A%2220f166be039fcc112-456011%22%2C%22uri%22%3A%22document_image%2F5045771b-83b4-4e49-a06f-8f219d439010-456011.jpg%22%7D%5D" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">5.<span class="bold underline" style="font-weight: bold; text-decoration: underline;"> ResNet</span>：ResNet 是 2015 年 ImageNet 竞赛的冠军，由微软研究院提出，通过残差模块能够成功地训练高达 152 层深的神经网络。ResNet 最初的设计灵感来自这个问题：在不断加深神经网络的时候，会出现一个 Degradation，即准确率会先上升然后达到饱和，再持续增加深度则会导致模型准确率下降。<span class="bold" style="font-weight: bold;">这并不是过拟合的问题</span>，因为不仅在验证集上误差增加，训练集本身误差也会增加。假设一个<span class="bold italic" style="font-weight: bold; font-style: italic;">比较浅的网络达到了饱和的准确率，那么在后面加上几个恒等映射层，误不会增加</span>，也就说更深的模型起码不会使得模型效果下降。这里提到的使用<span class="bold" style="font-weight: bold;">恒等映射</span>直接将前一层输出传到后面的思想，就是 ResNet 的灵感来源。假设某个神经网络的输人是 x，期望输出是H(x)，如果直接把输人x传到输出作为初始结果，那么此时需要学习的目标就是 F(x)=H(x)-x，也就是下面这个残差模型：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/5045771b-83b4-4e49-a06f-8f219d439010-456011.jpg" style="max-width: 720px; width: 485px;" class="attach-img"></div></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" images="%5B%7B%22w%22%3A510%2C%22ow%22%3A659%2C%22oh%22%3A273%2C%22id%22%3A%22159166c96ed9ba08-456011%22%2C%22uri%22%3A%22document_image%2Fd0412b41-3c01-400f-ab5d-541406e9cea2-456011.jpg%22%7D%5D" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">（9）其他内容补充：CNN发展史</span><span class="italic" style="font-style: italic;">（刘昕_CNN近期进展与实用技巧（上）：</span><a class="content-link" target="_blank" rel="noreferrer" href="https://zhuanlan.zhihu.com/p/21432547" style="text-decoration: underline; opacity: 0.6; color: inherit;"><span class="italic" style="font-style: italic;">https://zhuanlan.zhihu.com/p/21432547</span></a><span class="italic" style="font-style: italic;">）</span></span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/d0412b41-3c01-400f-ab5d-541406e9cea2-456011.jpg" style="max-width: 720px; width: 510px;" class="attach-img"></div><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">CNN 的演化路径可以总结为以下几个方向：</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">进化之路一：网络结构加深</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">进化之路二：加强卷积功能</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">进化之路三：从分类到检测</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">进化之路四：新增功能模块</span></li></ul></li></ul></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#ffaf38" style="color: rgb(255, 175, 56); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="italic" style="font-style: italic;">说明：《深度学习入门之PyTorch》该书偏深度学习实战，内容覆盖深度学习发展历史、深度学习框架、神经网络模型介绍及实战，推荐~</span></span></li></ul></li><li style="line-height: 24px;"><span class="content mubu-node" color="#dc2d1e" heading="3" style="color: rgb(220, 45, 30); line-height: 24px; min-height: 24px; font-size: 18px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">（六）其他一些内容</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">1）softmax函数</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A209%2C%22ow%22%3A365%2C%22oh%22%3A98%2C%22id%22%3A%2232d166c5328e9b0a5-456011%22%2C%22uri%22%3A%22document_image%2Fb2694f73-0e3c-4412-b5ef-80c414e4e8c9-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">我们知道 MNIST 的结果是 0-9，我们的模型可能推测出一张图片是数字 9 的概率是80%，是数字 8 的概率是 10%，然后其他数字的概率更小，总体概率加起来等于 1。这是一个使用 softmax 回归模型的经典案例。softmax 模型可以用来给不同的对象分配概率。</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/b2694f73-0e3c-4412-b5ef-80c414e4e8c9-456011.jpg" style="max-width: 720px; width: 209px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A292%2C%22ow%22%3A514%2C%22oh%22%3A184%2C%22id%22%3A%223e4166c53311d0015-456011%22%2C%22uri%22%3A%22document_image%2Fbf4b1f5c-62d0-4f59-be9a-9bac04ba1bd7-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">比如输出结果为[1,5,3]：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/bf4b1f5c-62d0-4f59-be9a-9bac04ba1bd7-456011.jpg" style="max-width: 720px; width: 292px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">softmax 多用于多分类神经网络输出。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">2）激活函数有哪些性质？</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1. 非线性：当激活函数是线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即 f(x)=x，就不满足这个性质，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的；</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2. 可微性：当优化方法是基于梯度的时候，就体现了该性质；</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">3. 单调性：&nbsp;当激活函数是单调的时候，单层网络能够保证是凸函数；</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">4. f(x)≈x：当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">5. 输出值的范围：当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的 Learning Rate。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">3）如何选择激活函数？</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好，就去使用它。以下是常见的选择情况：</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1、如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2、如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">3、sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">4、tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">5、ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者Leaky ReLu，再去尝试其他的激活函数。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">6、如果遇到了一些死的神经元，我们可以使用 Leaky ReLU 函数。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">4）卷积如何检测边缘信息？</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">可以先看阮一峰这篇文章，<span class="italic" style="font-style: italic;">如何识别图像边缘</span>：<a class="content-link" target="_blank" rel="noreferrer" href="http://www.ruanyifeng.com/blog/2016/07/edge-recognition.html" style="text-decoration: underline; opacity: 0.6; color: inherit;">http://www.ruanyifeng.com/blog/2016/07/edge-recognition.html</a></span></li><li style="line-height: 22px;"><span class="content mubu-node" images="%5B%7B%22w%22%3A494%2C%22ow%22%3A732%2C%22oh%22%3A359%2C%22id%22%3A%2224a166c839d349047-456011%22%2C%22uri%22%3A%22document_image%2F24b6f497-ad64-41a8-b99b-962ae810ba93-456011.jpg%22%7D%5D" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">再解释下，如下，最右边的图像为 6*6 图像，左半边全是白的，右半边全是灰的，检查其中间的分界线：（使用了中间的那个 3*3 的过滤器可以检出）</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/24b6f497-ad64-41a8-b99b-962ae810ba93-456011.jpg" style="max-width: 720px; width: 494px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">可以看到，最终得到的结果中间是一段白色，两边为灰色，于是垂直边缘被找到了。为什么呢？因为在 6*6 图像中红框标出来的部分，也就是图像中的分界线所在部分，与过滤器进行卷积，结果是 30。而在不是分界线的所有部分进行卷积，结果都为 0。在这个图中，白色的分界线很粗，那是因为 6*6 的图像实在太小了，若是换成 1000*1000的图像，我们会发现在最终结果中，分界线不粗且很明显。<span class="bold" style="font-weight: bold;">这就是检测物体垂直边缘的例子</span>，水平边缘的话只需将过滤器旋转 90 度。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">5）怎样才能减少卷积层参数量？</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" images="%5B%7B%22w%22%3A330%2C%22ow%22%3A411%2C%22oh%22%3A364%2C%22id%22%3A%2211b166c92b3b93039-456011%22%2C%22uri%22%3A%22document_image%2Ff2d679f5-cbb0-4dca-8abf-57836a6d5d86-456011.jpg%22%7D%5D" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">发明 GoogleNet 的团队发现，如果仅仅引入多个尺寸的卷积核，会带来大量的额外的参数，受到 Network In Network 中 1×1 卷积核的启发，为了解决这个问题，他们往 Inception 结构中<span class="bold" style="font-weight: bold;">加入了一些 1×1 的卷积核</span>。加入 1×1 卷积核的 Inception 结构：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/f2d679f5-cbb0-4dca-8abf-57836a6d5d86-456011.jpg" style="max-width: 720px; width: 330px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">根据上图，我们来做个对比计算，假设输入 feature map 的维度为 256 维，要求输出维度也是 256 维。有以下两种操作：</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">（1）256 维的输入直接经过一个 3×3×256 的卷积层，输出一个 256 维的 feature map，191 那么参数量为：256×3×3×256 = 589,824</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">（2）256 维的输入先经过一个 1×1×64 的卷积层，再经过一个 3×3×64 的卷积层，最后经过一个 1×1×256 的卷积层，输出 256 维，参数量为：256×1×1×64 + 64×3×3×64+64×1×1×256 = 69,632。足足把第一种操作的参数量降低到九分之一！</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1×1 卷积核也被认为是影响深远的操作，往后大型的网络为了降低参数量都会应用上 1×1 卷积核。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">6）full，same，valid 三种卷积后图像大小的计算公式？</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" images="%5B%7B%22w%22%3A459%2C%22ow%22%3A695%2C%22oh%22%3A269%2C%22id%22%3A%22364166c93a4eb204b-456011%22%2C%22uri%22%3A%22document_image%2F3d8d859e-93ab-4ebb-b617-fd3abe8885e3-456011.jpg%22%7D%5D" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1、full: 滑动步长为 1，图片大小为 N1xN1，卷积核大小为 N2xN2，卷积后图像大小：N1+N2-1 x N1+N2-1。</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/3d8d859e-93ab-4ebb-b617-fd3abe8885e3-456011.jpg" style="max-width: 720px; width: 459px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" images="%5B%7B%22w%22%3A456%2C%22ow%22%3A658%2C%22oh%22%3A247%2C%22id%22%3A%229f166c93aae570cc-456011%22%2C%22uri%22%3A%22document_image%2Fe72df9c0-b1c7-4fec-89fd-d80bb71dac34-456011.jpg%22%7D%5D" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2、same: 滑动步长为 1，图片大小为 N1xN1，卷积核大小为 N2xN2，卷积后图像大小：N1xN1。</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/e72df9c0-b1c7-4fec-89fd-d80bb71dac34-456011.jpg" style="max-width: 720px; width: 456px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" images="%5B%7B%22w%22%3A211%2C%22ow%22%3A402%2C%22oh%22%3A274%2C%22id%22%3A%225c166c93aea2c04-456011%22%2C%22uri%22%3A%22document_image%2F4cd4b094-5920-4caf-8ea7-36ac8b9522de-456011.jpg%22%7D%5D" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">3、valid:滑动步长为 S，图片大小为 N1xN1，卷积核大小为 N2xN2，卷积后图像大小：(N1-N2)/S+1 x (N1-N2)/S+1。</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/4cd4b094-5920-4caf-8ea7-36ac8b9522de-456011.jpg" style="max-width: 720px; width: 211px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">注：2）、3）、4）、5）、6）内容来源，GitHub 仓库——<span class="italic" style="font-style: italic;">深度学习500问</span>：<a class="content-link" target="_blank" rel="noreferrer" href="https://github.com/scutan90/DeepLearning-500-questions" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://github.com/scutan90/DeepLearning-500-questions</a></span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">7）目标检测</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">（待更...）</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">8）图像分割（语义分割&amp;实例分割）</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">FCN（全卷积神经网络）</span>：对于一般的分类 CNN 网络，如 VGG 和 Resnet，都会在网络的最后加入一些全连接层，经过 softmax 后就可以获得类别概率信息。但是这个概率信息是 1 维的，即只能标识整个图片的类别，不能标识每个像素点的类别，所以这种全连接方法不适用于图像分割。<span class="underline" style="text-decoration: underline;">而 FCN 提出可以把后面几个全连接都换成卷积，这样就可以获得一张 2 维的 feature map，后接 softmax 获得每个像素点的分类信息，从而解决了分割问题</span>。</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A409%2C%22ow%22%3A693%2C%22oh%22%3A352%2C%22id%22%3A%22151166c9ecdb52164-456011%22%2C%22uri%22%3A%22document_image%2Fcde58266-1296-4c90-addc-bb5b7702f734-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">FCN 对图像进行像素级的分类，从而解决了语义级别的图像分割（semantic segmentation）问题。与经典的 CNN 在卷积层之后使用全连接层得到固定长度的特征向量进行分类（全联接层＋softmax 输出）不同，FCN 可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的 feature map 进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。下图是语义分割所采用的全卷积网络(FCN)的结构示意图：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/cde58266-1296-4c90-addc-bb5b7702f734-456011.jpg" style="max-width: 720px; width: 409px;" class="attach-img"></div></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">（待更...）</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">9）为何深度学习这么强？</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">1. 不需要人工提取特征：深度学习是一个 End-to-End 的训练方式，不需要人工对训练样本进行高质量的特征提取然后向量化，只需要做很少的归一、白化处理，直接把训练样本扔给机器学习就好，因为在神经网络中，由于海量的线性分类器的堆叠（并行和串行）以及卷积网络的使用，它对噪声的容忍能力、对多通道数据上投射出来的不同特征偏向的敏感程度会自动重视或忽略。人工参与越少？我们越接近人工智能？</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">2. 处理线性不可分：通过大量线性分类器的堆叠使得整体上可以将线性不可分的问题变得可分，每个神经元其实就是一个线性分类器，神经网络能且只能通过线性分类器的组合解决线性不可分。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="italic" style="font-style: italic;">参考：①What's Deep Learning and why it is so powerful ：</span><a class="content-link" target="_blank" rel="noreferrer" href="https://github.com/GodCoder/Blog.me/issues/14" style="text-decoration: underline; opacity: 0.6; color: inherit;"><span class="italic" style="font-style: italic;">https://github.com/GodCoder/Blog.me/issues/14</span></a><span class="italic" style="font-style: italic;">；②书籍《白话深度学习与TensorFlow》2.3节</span></span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">10）为什么说神经网络是个黑箱？</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A500%2C%22ow%22%3A812%2C%22oh%22%3A198%2C%22id%22%3A%223c8166ca6289a111b%22%2C%22uri%22%3A%22document_image%2F9a04bdf7-8266-4d38-b5d2-7290909da845-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">可以看下图体会下：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/9a04bdf7-8266-4d38-b5d2-7290909da845-456011.jpg" style="max-width: 720px; width: 500px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="italic" style="font-style: italic;">相关文章：①知乎「为什么都说神经网络是个黑箱？」：</span><a class="content-link" target="_blank" rel="noreferrer" href="https://www.zhihu.com/question/263672028" style="text-decoration: underline; opacity: 0.6; color: inherit;"><span class="italic" style="font-style: italic;">https://www.zhihu.com/question/263672028</span></a></span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" images="%5B%7B%22w%22%3A368%2C%22ow%22%3A780%2C%22oh%22%3A684%2C%22id%22%3A%22310166ca30ab1e09c-456011%22%2C%22uri%22%3A%22document_image%2F95f5bbf4-6fab-45b0-b6ab-78ad637e629a-456011.jpg%22%7D%5D" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">11）什么是 end-to-end（端到端）神经网络？</span></span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/95f5bbf4-6fab-45b0-b6ab-78ad637e629a-456011.jpg" style="max-width: 720px; width: 368px;" class="attach-img"></div><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="italic" style="font-style: italic;">参考：知乎「什么是 end-to-end 神经网络？」：</span><a class="content-link" target="_blank" rel="noreferrer" href="https://www.zhihu.com/question/51435499" style="text-decoration: underline; opacity: 0.6; color: inherit;"><span class="italic" style="font-style: italic;">https://www.zhihu.com/question/51435499</span></a></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">我的小结：就是输入是原始数据，输出是最后的结果，只关心输入和输出，中间的步骤全部都不管。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">12）为什么激活函数都采用非线性函数，不采用线性激活函数？</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A220%2C%22ow%22%3A529%2C%22oh%22%3A407%2C%22id%22%3A%221e2166ca4db0d7024-456011%22%2C%22uri%22%3A%22document_image%2F5b3ffe95-0dcb-4ade-97f4-f10d97f0eeb0-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">如要将下面的三角形和圆形点进行正确的分类：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/5b3ffe95-0dcb-4ade-97f4-f10d97f0eeb0-456011.jpg" style="max-width: 720px; width: 220px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A305%2C%22ow%22%3A720%2C%22oh%22%3A375%2C%22id%22%3A%221e7166ca4e2e3d181-456011%22%2C%22uri%22%3A%22document_image%2F56cdfa3a-35b5-45df-aa04-0b845dbdab93-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">利用我们单层的感知机，用它可以划出一条线，把平面分割开：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/56cdfa3a-35b5-45df-aa04-0b845dbdab93-456011.jpg" style="max-width: 720px; width: 305px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A350%2C%22ow%22%3A720%2C%22oh%22%3A331%2C%22id%22%3A%22239166ca5094be009-456011%22%2C%22uri%22%3A%22document_image%2F1eceaaaf-03fd-4176-9bc9-591de479d715-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">不管怎么分，单个直线都无法分开，那么我们很容易想到用多个感知器来进行组合，以便获得更大的分类问题，好的，下面我们上图，看是否可行：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/1eceaaaf-03fd-4176-9bc9-591de479d715-456011.jpg" style="max-width: 720px; width: 350px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A601%2C%22ow%22%3A967%2C%22oh%22%3A26%2C%22id%22%3A%221b4166ca5143c2013-456011%22%2C%22uri%22%3A%22document_image%2Fe860365d-ace2-43ca-ba55-40e477e5190c-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">我们已经得到了多感知器分类器了，那么它的分类能力是否强大到能将非线性数据点正确分类开呢，分析一下：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/e860365d-ace2-43ca-ba55-40e477e5190c-456011.jpg" style="max-width: 720px; width: 601px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A219%2C%22ow%22%3A619%2C%22oh%22%3A442%2C%22id%22%3A%22214166ca51c46107b-456011%22%2C%22uri%22%3A%22document_image%2Fe747e672-7d34-4c61-9a82-e7150fec55a7-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">能看出，不管它怎么组合，最多就是线性方程的组合，最后得到的分类器本质还是一个线性方程，该处理不了的非线性问题，它还是处理不了。就好像下图，直线无论在平面上如果旋转，都不可能完全正确的分开三角形和圆形点：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/e747e672-7d34-4c61-9a82-e7150fec55a7-456011.jpg" style="max-width: 720px; width: 219px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A353%2C%22ow%22%3A643%2C%22oh%22%3A298%2C%22id%22%3A%2238d166ca53008c05-456011%22%2C%22uri%22%3A%22document_image%2Fd4b9e429-d283-471d-9c4d-35a6ce612226-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">那么抛开神经网络中神经元需不需要激活函数这点不说，如果没有激活函数，仅仅是线性函数的组合解决的问题太有限了，碰到非线性问题就束手无策了。那么加入激活函数是否可能能够解决呢？在上面线性方程的组合过程中，我们其实类似在做三条直线的组合，如下图：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/d4b9e429-d283-471d-9c4d-35a6ce612226-456011.jpg" style="max-width: 720px; width: 353px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="italic" style="font-style: italic;">忆臻的回答 - 知乎：</span><a class="content-link" target="_blank" rel="noreferrer" href="https://www.zhihu.com/question/30165798/answer/147546271" style="text-decoration: underline; opacity: 0.6; color: inherit;"><span class="italic" style="font-style: italic;">https://www.zhihu.com/question/30165798/answer/147546271</span></a></span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#3da8f5" style="color: rgb(61, 168, 245); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">13）理解CRF（条件随机场）、MRF（马尔科夫随机场）</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">条件随机场：</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A307%2C%22ow%22%3A521%2C%22oh%22%3A339%2C%22id%22%3A%226e166ca68b99811e-456011%22%2C%22uri%22%3A%22document_image%2F89b94c1e-e2db-4aa6-b7fc-165e33974831-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">简单来介绍一下 “条件随机场” 的概念。FCN 是像素到像素的影射，所以最终输出的图片上每一个像素都是标注了分类的，将这些分类简单地看成是不同的变量，每个像素都和其他像素之间建立一种连接，连接就是相互间的关系。于是就会得到一个 “完全图”：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/89b94c1e-e2db-4aa6-b7fc-165e33974831-456011.jpg" style="max-width: 720px; width: 307px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" images="%5B%7B%22w%22%3A201%2C%22ow%22%3A260%2C%22oh%22%3A52%2C%22id%22%3A%221fa166ca691fc3182-456011%22%2C%22uri%22%3A%22document_image%2F765b768d-1c4f-4d45-ada4-ff6f15846150-456011.jpg%22%7D%5D" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">上图是以 4*6 大小的图像像素阵列表示的简易版。那么在全链接的 CRF 模型中，有一个对应的能量函数：</span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/765b768d-1c4f-4d45-ada4-ff6f15846150-456011.jpg" style="max-width: 720px; width: 201px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">嗯，不要问我这个公式里各种符号是啥，我看不懂。但是我知道这个公式是干嘛滴：其中等号右边第一个一元项，表示像素对应的语义类别，其类别可以由 FCN 或者其他语义分割模型的预测结果得到；而第二项为二元项，二元项可将像素之间的语义联系/关系考虑进去。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">这么说太抽象，举个简单的例子：<span class="bold" style="font-weight: bold;">“天空”和 “鸟” 这样的像素在物理空间是相邻的概率，应该要比 “天空” 和 “鱼” 这样像素相邻的概率大，那么天空的边缘就更应该判断为鸟而不是鱼（从概率的角度）</span>。通过对这个能量函数优化求解，把明显不符合事实识别判断剔除，替换成合理的解释，得到对 FCN 的图像语义预测结果的优化，生成最终的语义分割结果。</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="italic" style="font-style: italic;">参考来源_十分钟看懂图像语义分割技术：</span><a class="content-link" target="_blank" rel="noreferrer" href="https://www.leiphone.com/news/201705/YbRHBVIjhqVBP0X5.html" style="text-decoration: underline; opacity: 0.6; color: inherit;"><span class="italic" style="font-style: italic;">https://www.leiphone.com/news/201705/YbRHBVIjhqVBP0X5.html</span></a><span class="italic" style="font-style: italic;">（一篇很干货的关于语义分割的综述，推荐~）</span></span></li></ul></li></ul></li></ul></li></ul></li><li style="line-height: 28px;"><span class="content mubu-node" heading="2" style="line-height: 28px; min-height: 28px; font-size: 21px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">非监督学习</span></span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">聚类 --&gt; k-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。</span></li></ul></li><li style="line-height: 28px;"><span class="content mubu-node" heading="2" style="line-height: 28px; min-height: 28px; font-size: 21px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">半监督学习</span></span></li><li style="line-height: 28px;"><span class="content mubu-node" color="#333333" heading="2" style="color: rgb(51, 51, 51); line-height: 28px; min-height: 28px; font-size: 21px; display: inline-block; vertical-align: top;"><span class="bold" style="font-weight: bold;">强化学习（增强学习）</span></span></li><li style="line-height: 24px;"><span class="content mubu-node" color="#333333" heading="3" style="color: rgb(51, 51, 51); line-height: 24px; min-height: 24px; font-size: 18px; display: inline-block; vertical-align: top;">说明：</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">吴恩达机器学习视频课程，coursera地址：<a class="content-link" target="_blank" rel="noreferrer" href="https://www.coursera.org/learn/machine-learning" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://www.coursera.org/learn/machine-learning</a></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">黄海广博士等人对该视频写了一份笔记，地址：<a class="content-link" target="_blank" rel="noreferrer" href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">建议初学者：打印笔记 pdf，边看视频边涂画，顺便可以在打印的笔记中的空白处补充自己网上 get 到的内容~</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">注：主要为对吴恩达机器学习课程要点稍微梳理下，另外深度学习内容来源《Python深学习》、《深度学习入门之PyTorch》书籍~</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">update：2018-10-31</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">相关资料汇总：</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[Video]偏科普入门，莫烦机器学习教程：<a class="content-link" target="_blank" rel="noreferrer" href="https://morvanzhou.github.io/tutorials/machine-learning/" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://morvanzhou.github.io/tutorials/machine-learning/</a></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[Video]适合入门，吴恩达机器学习课程：<a class="content-link" target="_blank" rel="noreferrer" href="https://www.coursera.org/learn/machine-learning" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://www.coursera.org/learn/machine-learning</a>、或 B 站：<a class="content-link" target="_blank" rel="noreferrer" href="https://www.bilibili.com/video/av9912938/" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://www.bilibili.com/video/av9912938/</a></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[Video]吴恩达深度学习课程：<a class="content-link" target="_blank" rel="noreferrer" href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://mooc.study.163.com/smartSpec/detail/1001319001.htm</a>（中英文字幕）</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[Video]林轩田《机器学习基石》，B 站观看：<a class="content-link" target="_blank" rel="noreferrer" href="https://www.bilibili.com/video/av1624332" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://www.bilibili.com/video/av1624332</a></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[Video]林轩田《机器学习技法》，B 站观看：<a class="content-link" target="_blank" rel="noreferrer" href="https://www.bilibili.com/video/av12469267/" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://www.bilibili.com/video/av12469267/</a></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[Video]李宏毅《一天搞懂深度学习》，B 站观看：<a class="content-link" target="_blank" rel="noreferrer" href="https://www.bilibili.com/video/av16543434/" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://www.bilibili.com/video/av16543434/</a></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[Video]李宏毅_机器学习，B 站观看：<a class="content-link" target="_blank" rel="noreferrer" href="https://www.bilibili.com/video/av10590361/" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://www.bilibili.com/video/av10590361/</a></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[Video]李宏毅_深度学习，B 站观看：<a class="content-link" target="_blank" rel="noreferrer" href="https://www.bilibili.com/video/av9770302/" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://www.bilibili.com/video/av9770302/</a></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[Video]深度学习计算机视觉课程，李飞飞_斯坦福 CS231n 课程，B 站观看：<a class="content-link" target="_blank" rel="noreferrer" href="https://www.bilibili.com/video/av13260183/" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://www.bilibili.com/video/av13260183/</a>（中文字幕）</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[ML_Books]周志华《机器学习》("西瓜书")、李航《统计学方法》、Peter Harrington《机器学习实战》、</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[DL_Books]Ian Goodfellow《深度学习》("花书")、Michael Nielsen《Neural Network and Deep Learning》、弗朗索瓦·肖莱&nbsp;《Python深度学习》、廖星宇《深度学习入门之PyTorch》、张玉宏《深度学习之美:AI时代的数据处理与最佳实践》等       </span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[DL_Blog]阮一峰_神经网络入门：<a class="content-link" target="_blank" rel="noreferrer" href="http://www.ruanyifeng.com/blog/2017/07/neural-network.html" style="text-decoration: underline; opacity: 0.6; color: inherit;">http://www.ruanyifeng.com/blog/2017/07/neural-network.html</a></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[DL_Blog]阮一峰_如何识别图像边缘：<a class="content-link" target="_blank" rel="noreferrer" href="http://www.ruanyifeng.com/blog/2016/07/edge-recognition.html" style="text-decoration: underline; opacity: 0.6; color: inherit;">http://www.ruanyifeng.com/blog/2016/07/edge-recognition.html</a></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[DL_Blog]《零基础入门深度学习》系列文章：<a class="content-link" target="_blank" rel="noreferrer" href="https://www.zybuluo.com/hanbingtao/note/433855" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://www.zybuluo.com/hanbingtao/note/433855</a></span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">[Notice]这一两年关于人工智能的相关书籍、视频各种资料真的是太多了。资料太多有时反而不是好事，特别对准备入门的伙伴。表示对于本人有<span class="bold underline" style="font-weight: bold; text-decoration: underline;">看过的以及有稍微浏览和耳闻不错的资料</span>推荐下吧：</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">完全小白，先看看莫烦的教程吧</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">机器学习入门可看看吴恩达机器学习视频课程，打印黄海广等人写的笔记，然后可以啃《机器学习》、《统计学方法》，想多实战可以看《机器学习实战》</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">深度学习入门可以先看看阮一峰「<span class="italic" style="font-style: italic;">神经网络入门</span>」，「<span class="italic" style="font-style: italic;">如何识别图像边缘</span>」文章，再找这本《Neural Network and Deep Learning》书的中文版看，再看《零基础入门深度学习》系列文章，然后像《Python深度学习》、《深度学习入门之PyTorch》等书都可以看看</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">另外，关于卷积神经网络CNN，可以看看「<span class="italic" style="font-style: italic;">【深度学习系列】卷积神经网络CNN原理详解(一)——基本原理</span>」：<a class="content-link" target="_blank" rel="noreferrer" href="https://www.cnblogs.com/charlotte77/p/7759802.html" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://www.cnblogs.com/charlotte77/p/7759802.html</a>、关于反向传播「<span class="italic" style="font-style: italic;">一文弄懂神经网络中的反向传播法——BackPropagation</span>」：<a class="content-link" target="_blank" rel="noreferrer" href="https://www.cnblogs.com/charlotte77/p/5629865.html" style="text-decoration: underline; opacity: 0.6; color: inherit;">https://www.cnblogs.com/charlotte77/p/5629865.html</a> 等等，网上文章很多的</span></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">计算机视觉课程，视频可以看李飞飞斯坦福 CS231n 课程，然后就是找网上博客了。</span></li></ul></li></ul></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" color="#333333" style="color: rgb(51, 51, 51); line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"><span class="italic" style="font-style: italic;">By 幕布制作。通过该邀请链接注册：</span><a class="content-link" target="_blank" rel="noreferrer" href="https://mubu.com/inv/456011" style="text-decoration: underline; opacity: 0.6; color: inherit;"><span class="italic" style="font-style: italic;">https://mubu.com/inv/456011</span></a><span class="italic" style="font-style: italic;">，你将免费获赠15天幕布高级版。</span></span></li></ul></div>

</body>
</html>